<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>colourcats | Arash Akbarinia</title> <meta name="author" content="Arash Akbarinia"> <meta name="description" content="Deep Reconciliation of Categorical Colour perception"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://arashakbarinia.github.io/projects/DeepReconciliationOfCategoricalColourPerception/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Arash </span>Akbarinia</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">colourcats</h1> <p class="post-description">Deep Reconciliation of Categorical Colour perception</p> </header> <article> <h1 id="deep-reconciliation-of-categorical-colour-perception">Deep Reconciliation of Categorical Colour perception</h1> <p>We perceive colours categorically. Our perceptual system separates a continuous space into distinct categories. The most prominent example is the rainbow, there are no discontinuities in its colour spectrum, yet we see discrete bands. The underlying reason, particularly the role of language, has spawned a heated debate between universalists and relativists. We reconcile these two explanations by studying vision-language and pure-vision deep neural networks (DNN). The results of our odd-one-out experiments show that pure-vision models (e.g., ImageNet object recognition networks) explain 85% of human data. In turn, suggesting a large part of our categorical colour perception is purely driven by visual signals. The remaining 15% is explained with vision-language models (e.g., CLIP text-image matching networks) even when tested without their language module. In turn, suggesting colour categories is a free-from-language representation, yet linguistic colour terms have influenced its development. We investigated whether colour categories emerge in all pure-vision models by studying Taskonomy networks trained on 24 visual tasks. Human-like colour categories appear only in less than half of those models, namely, networks trained on semantic (e.g., image segmentation, object recognition, and scene classification) or 3D tasks (e.g., shade from shaping, surface normal prediction, and depth estimation). Our results show low-level tasks (e.g., autoencoding and denoising) never obtain human-like colour categories. It also matters whether a network is trained on 2- or 3-dimensional outputs for the same perceptual task. Networks trained on 3D tasks of edge and keypoint detection obtain human-like colour categories but not their corresponding 2D networks. Overall, our findings provide evidence for the utility of categorical colour representation in several visual tasks but also indicating a portion of categorical colour perception can only be explained by the language component, reconciling both universal and relative theories.</p> <p align="center"> <a href="https://github.com/ArashAkbarinia/DeepTHS" rel="external nofollow noopener" target="_blank"><img src="https://github.com/ArashAkbarinia/arashakbarinia.github.io/blob/master/assets/img/github_icon.png?raw=true" alt="Source Code" style="height:100px;"></a> <a href="https://colab.research.google.com/github/ArashAkbarinia/arashakbarinia.github.io/blob/master/notebooks/DeepReconciliationOfCategoricalColourPerception.ipynb" rel="external nofollow noopener" target="_blank"><img src="https://github.com/ArashAkbarinia/arashakbarinia.github.io/blob/master/assets/img/colab_icon.png?raw=true" alt="Notebook" style="height:100px;"></a> </p> <ol> <li> <a href="#rainbow">Rainbow</a> <ul> <li><a href="#hue-circle-in-hsv-colour-space">Hue circle</a></li> <li><a href="#open-questions">Open questions</a></li> </ul> </li> <li> <a href="#stimuli">Stimuli</a> <ul> <li><a href="#munsell-chips">Munsell chips</a></li> <li><a href="#human-data">Human data</a></li> <li><a href="#shapes">Shapes</a></li> </ul> </li> <li> <a href="#experiments">Experiments</a> <ul> <li><a href="#language-models-zero-shot-evaluation">Language models</a></li> <li> <a href="#vision-models-odd-one-out-linear-classifier">Vision models</a> <ul> <li><a href="#training-colour-discriminator">Training colour discriminator</a></li> <li><a href="#testing-paradigm">Testing paradigm</a></li> </ul> </li> <li><a href="#pretrained-networks">Pretrained networks</a></li> </ul> </li> <li> <a href="#results">Results</a> <ul> <li><a href="#explaining-with-one-example">One example</a></li> <li><a href="#baseline--rgb-model">Baseline</a></li> <li> <a href="#the-role-of-language">The role of language</a> <ul> <li><a href="#clip---vit-b32">CLIP – ViT-B32</a></li> <li><a href="#clip---resnet50">CLIP – ResNet50</a></li> <li><a href="#imagenet---vit-b32">ImageNet – ViT-B32</a></li> <li><a href="#imagenet---resnet50">ImageNet – ResNet50</a></li> <li><a href="#multimodal-language-vision-vs-unimodal-vision">Multimodal language-vision vs. unimodal vision</a></li> </ul> </li> <li> <a href="#the-role-of-visual-task">The role of visual tasks</a> <ul> <li><a href="#munsell-prediction">Munsell prediction</a></li> <li><a href="#comparison-across-tasks">Comparison across tasks</a></li> </ul> </li> </ul> </li> <li><a href="#discussion">Discussion</a></li> </ol> <h2 id="rainbow">Rainbow</h2> <p>There are no discontinuities in the electromagnetic spectrum of the light reaching us from a rainbow and yet we see hues clearly separated by colour categories.</p> <p>To make this more tangible, we can simulate a rainbow and check the numerical values of its constituent hues.</p> <h3 id="hue-circle-in-hsv-colour-space">Hue circle in HSV colour space</h3> <p>We create a list of 180 hues with constant saturation and values.</p> <p>We convert the <code class="language-plaintext highlighter-rouge">rainbow_hues</code> from <em>HSV</em> colour space to <em>RGB</em> for visualisation purposes. We can observe that the continuous function of hues appears to be separated into different categories. This separation stems from our perceptual system otherwise the HSV colour space is only an alternative representation of the RGB space that is obtained from camera sensitivity functions.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_15_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_15_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_15_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_15_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="coloured-arches">Coloured arches</h3> <p>The visualisation of the hue circle does not have a spatial resolution (i.e., only one row), to better appreciate the categorical colour phenomenon, we convert the hue circle into an image of coloured arches, similar to natural rainbow.</p> <p>When we illustrate coloured arches, we can clearly see colour categories such as (from bottom to top):</p> <ol> <li>pink</li> <li>purple</li> <li>blue</li> <li>cyan</li> <li>green</li> <li>yellow</li> <li>orange</li> <li>red</li> </ol> <p>Although to different observers the actual colour names and the number of distinct colour categories might differ, but the principle remains the same, everyone sees the continuous function in discrete categories.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_19_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_19_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_19_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_19_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="rgb-prediction">RGB prediction</h4> <p>We can pass this rainbow image to a computational model and check its categorisation. The simplest model is the Euclidean distance towards focal colour categories. We consider two types of <em>focal colours</em>:</p> <ol> <li>Uniformly selected within the rainbow image.</li> <li>Focal colour categories from human data.</li> </ol> <p>We can observe that this model created bands of equal width when the <em>focal colours</em> are selected uniformly. This is rather trivial. There is no categorical effect in a continuous space. Nevertheless, this reemphasises that any categorical colours we perceive in the rainbow image are purely driven by our visual system.</p> <p>In the second scenario, where focal colours are selected from human data, we observe that the RGB prediction is not uniform but still does not resemble our perception. For instance, The blue and orange categories are disproportionally large suggesting the computation behind our categorical colour perception is not a simple smallest distance in the input space and some nonlinearity is necessary.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/rgb_rainbow-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/rgb_rainbow-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/rgb_rainbow-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/rgb_rainbow.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="open-questions">Open questions</h3> <p>This is a fascinating phenomenon that raises the following questions:</p> <ol> <li>Why do we perceive a continuous function in different categories?</li> <li>Are these categories shared among everyone or are there significant individual differences?</li> <li>Is the categorical colour perception because of linguistic terms we assign to colours and/or this is purely driven by our visual system?</li> </ol> <p>In this project, we used deep neural networks as a framework to study some aspects of categorical colour perception:</p> <ol> <li>We compare <strong>multimodal vision-language</strong> to <strong>unimodal vision</strong> deep neural networks to investigate <strong>the role of language</strong>.</li> <li>We compare 24 networks pretrained on an identical dataset for 24 distinct tasks to investigate <strong>the role of visual tasks</strong>.</li> <li>We compare two <strong>convolutional neural networks (CNN)</strong> to <strong>vision transformers (ViT)</strong> to investigate <strong>the role of architecture</strong>.</li> <li>We compare representation at different <strong>intermediate layers</strong> to investigate whether this is <strong>low-, mid, or high-level</strong> representation.</li> </ol> <h2 id="stimuli">Stimuli</h2> <p>In this section we describe the stimuli set we used to evaluate networks.</p> <h3 id="munsell-chips">Munsell chips</h3> <p>We use 320 <em>Munsell chips</em> to evaluate our networks. These chips have been extensively used in the literature of categorical colour perception.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_24_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_24_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_24_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_24_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="human-data">Human data</h3> <p>We rely on human-data for <strong>8</strong> chromatic colour categories <em>(Berlin &amp; Kay, 1969; Sturges &amp; Whitfield, 1995)</em>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_26_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_26_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_26_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_26_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_27_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_27_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_27_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_27_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="shapes">Shapes</h3> <p>We generated 2905 shapes by systematically changing five parameters of superellipse equation:</p> \[r = \left( |\frac{cos(\theta)}{a}|^{n} + |\frac{sin(\theta)}{b}|^{n} \right)^{\frac{-1}{n}}\] <p>The idea behind using a systematic geometrical shape is to investigate the interaction between object shape and colour perception. This is beyond the scope of this notebook and is part of another project.</p> <p>We can plot 25 of those superellipses randomly.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_30_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_30_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_30_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_30_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="experiments">Experiments</h2> <h3 id="language-models-zero-shot-evaluation">Language models: zero-shot evaluation</h3> <p>For the language models (i.e., <strong>CLIP</strong>) we can directly conduct psychophysics on the network without any intermediate step:</p> <ul> <li>For each shape, we colour it with a Munsell chip and input the network with eight phrases corresponding to eight colour terms.</li> <li>The phrase: <strong>“This is a {COLOUR-TERM} shape.”</strong> and <em>COLOUR-TERM</em> is one of these terms: <strong>red, orange, yellow, brown, green, blue, purple, pink</strong>.</li> <li>Network’s output is a number (the probability of the phrase matching the image). We take the phrase with the highest probability as the network’s final output.</li> <li>For each Munsell chip, we repeat this procedure for all shapes (2905), performing <strong>23,240 trials</strong> (\(8\times 2905\)).</li> </ul> <p>Let’s look at one example with simulated data. In this example, the network’s output is highest for the phrase <strong>“This is a red shape.”</strong>, therefore we take that as the network has placed this colour into the red category. The average across all 2905 shapes would be the final prediction of the network for this colour.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_33_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_33_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_33_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_33_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="vision-models-odd-one-out-linear-classifier">Vision models: odd-one-out linear classifier</h3> <p>It is impossible to directly ask a neural network trained on a task like object recognition about colour categories, as the neural network was specifically trained for another task. To overcome this, we trained a linear classifier to perform a 4AFC colour discrimination task and at test time evaluate whether this discrimination is categorical. That is to say, the framework to evaluate the categorical colour perception in vision models consists of two steps:</p> <ol> <li>A network is trained on an arbitrary visual task (e.g., object recognition). We refer to such a network as a <strong>pretrained network</strong>.</li> <li>Features extracted from the <strong>frozen</strong> pretrained network are input to a linear classifier trained for the colour discrimination 4AFC task. We refer to the trained linear classifier as a <strong>colour-discriminator</strong>.</li> </ol> <h4 id="training-colour-discriminator">Training colour-discriminator</h4> <p>The figure below shows the schematics of our training process.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colour_discriminator-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colour_discriminator-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colour_discriminator-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/colour_discriminator.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The process of extracting features (also known as, readouts) from a pretrained network can occur at any depth of a network. We extract features from six distinct layers from the early to final layer:</p> <ul> <li>Common to all architectures: <code class="language-plaintext highlighter-rouge">fc</code> for <em>ImageNet</em> (classification layer) or <code class="language-plaintext highlighter-rouge">encoder</code> for <em>Taskonomy</em> (the final encoding layer) and <em>CLIP</em> (the final vision layer).</li> <li>In the case of <code class="language-plaintext highlighter-rouge">ResNet50</code> architecture, from 5 intermediate areas (a collection of residual blocks).</li> <li>In the case of <code class="language-plaintext highlighter-rouge">ViT-B32</code> from blocks <code class="language-plaintext highlighter-rouge">[1, 4, 7, 10, 11]</code>.</li> </ul> <h4 id="train-images">Train images</h4> <p>During the training, the linear classifier is input with four images:</p> <ul> <li>Three of those are identical.</li> <li>One odd image that only differs in colour.</li> </ul> <p>The colour difference between common-odd images is drawn from a random uniform distribution ensuring no colour bias is introduced in the colour discriminator training.</p> <p>The background colour is always achromatic whose luminance is drawn from a random uniform distribution</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_37_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_37_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_37_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_37_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="testing-paradigm">Testing paradigm</h4> <p>At test time, for each Munsell chip, we evaluate against all sets of focal colour pairs. For instance, for the <code class="language-plaintext highlighter-rouge">munsell_chips[3][2]</code>, we input the network with the following four colours:</p> <ul> <li>The first and last being one focal colour (in this example pink and red categories).</li> <li>The two middle images are identical to the colour of the test chip.</li> </ul> <p>We interpret the results:</p> <ol> <li>If the network outputs the pink image as odd, we conclude that the network has put the test chip into the <strong>red category</strong>.</li> <li>If the network outputs the red image as odd, we conclude that the network has put the test chip in the <strong>pink category</strong>.</li> </ol> <p>We also test the network by swapping the first and last images to ensure the results are not biased towards a certain index.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_39_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_39_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_39_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_39_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>For the same test chip, we repeat this procedure for all 28 combinations of pairs of focal colours:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/test_paradigm-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/test_paradigm-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/test_paradigm-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/test_paradigm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We assign the colour that wins the most trials across all tests/shapes as the colour of the test chip.</p> <h3 id="pretrained-networks">Pretrained networks</h3> <p>Architectures:</p> <ul> <li> <strong>Vision Transformers (ViT)</strong> – <em>ViT-B32</em> </li> <li> <strong>Convolutional Neural Networks (CNN)</strong> – <em>ResNet50</em> </li> </ul> <p>Pretrained task:</p> <ul> <li> <strong>CLIP</strong>: multimodal text-image matching</li> <li> <strong>ImageNet</strong>: unimodal object classification</li> <li> <strong>Taskonomy</strong>: 24 unimodal tasks (e.g., scene segmentation, object recognition, edge detection, denoising, etc.).</li> </ul> <p>Inetrmediate layers: six distinct layers corresponding to low-, mid- and high-level visual representation.</p> <h2 id="results">Results</h2> <p>To better understand the data crunch from raw data to plotted figures, we look at one example.</p> <h3 id="explaining-with-one-example">Explaining with one example</h3> <p>We will look at the results of <em>Block-10</em> of the <em>ViT-B32</em> architecture (i.e., the image encoder of CLIP). The directory name <code class="language-plaintext highlighter-rouge">bg128_i0</code> means the linear classifier (colour discriminator) has been trained with images of grey background (\(R=G=B=127\)).</p> <p>The results are loaded into an array of shape <code class="language-plaintext highlighter-rouge">(320, 8, 8)</code>: essentially the 320 confusion matrices of size \(8 \times 8\).</p> <h4 id="one-test-chip">One Test chip</h4> <p>We can simply print the confusion matrix for one particular Munsell chip. We do that for <strong>chip at index 122</strong> which corresponds to the test colour we visualised when explaining the testing paradigm:</p> <ol> <li>Each cell is the percentage of time a focal colour was selected as the odd element (i.e., lost the categorical battle to another focal colour).</li> <li>The diagonal of the matrix is 0.</li> <li>Higher values indicate strong categorical representation (e.g., orange versus all other colours). A value close to 0.50 indicates a chance level and no categorical representation.</li> <li>The confusion matrix cannot be reduced to the (upper/lower) triangle. The sum of \(C + C^T \leq 1\), where \(C\) and \(C^T\) denote the confusion matrix and its transpose. Their sum is not guaranteed to equal 1, as the selected odd index by the network can also be the identical test chip. Small numbers for \(C + C^T\) indicate the network’s representation is not categorical. Overall, it can be observed that the \(C + C^T\) is quite close to 1.</li> </ol> <p>We compute the <em>average</em> winning rate of each focal colour and the highest value is selected as the colour of the test chip.:</p> <ul> <li>In this example <em>orange</em> wins against other focal colours \(95\%\) of the time.</li> <li>Note that the <em>average</em> column is not a true probability distribution, the sum is not equal to 1.</li> <li>A value of 0.50 indicates no categorical representation.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_50_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_50_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_50_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_50_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="all-chips">All chips</h4> <p>We compute the colour of each Munsell chip following the procedure above. We can plot this in the same format as the original Munsell image.</p> <p>The plotted figure can be interpreted as follows:</p> <ul> <li>Those cells with a white colour do not have ground truth, therefore excluded.</li> <li>Other cells have taken a focal colour according to the network prediction.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_55_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_55_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_55_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_55_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We can compute the accuracy with respect to the ground truth. In this example, the network predicts human data \(83\%\) of time.</p> <p>We plot <strong>x</strong> on top of the cells where the network’s prediction mismatch the human data:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_60_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_60_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_60_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_60_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="baseline--rgb-model">Baseline – RGB Model</h3> <p>To put the obtained results with networks into perspective we compare them to the “RGB Model” that assigns the closest colour category (smallest Euclidean distance) to a Munsell chip:</p> <ol> <li>Eight focal colours are selected by taking the average of all Munsell chips in the Berlin &amp; Kay (1969) and Sturges &amp; Whitfield (1995) Ground Truth.</li> <li>For each Munsell chip, we compute the Euclidean distance to eight focal colours.</li> <li>The focal colour with the smallest Euclidean distance is selected as the colour category of that test chip.</li> </ol> <p>In simple words, if the test chip is physically closest in the space to the pink category, its colour category will be pink.</p> <p>One might argue that step 0 is not an accurate estimation of focal colours as the centre of mass does not necessarily determine the focal colour. Our rationale behind this baseline:</p> <ul> <li>The RGB Model categorises colours uniformly based on proximity.</li> <li>Essentially, in any colour space there is an inherent categorical effect of uniform discretisation.</li> <li>Therefore, any categorical effect we observe in the network is significant if it surpasses the RGB Model.</li> </ul> <p>We computed the physical proximity in several colour spaces, including:</p> <ul> <li>DKL</li> <li>CIE L*a*b*</li> <li>RGB</li> <li>LMS</li> <li>HSV</li> <li>YP<sub>b</sub>P<sub>r</sub> </li> </ul> <p>We have taken RGB as the baseline because of two reasons:</p> <ul> <li>The pattern of results across colour spaces is very similar.</li> <li>The input colour space to networks is RGB (i.e., the pretrained datasets are in RGB). Therefore we can analyse how much the colour categories alter from the input representation to the network’s internal representation.</li> </ul> <p>Below, we have visualised the prediction from six different colour spaces. We can observe that although simple Euclidean distance in colour spaces can predict the human data to a certain extent, the error rate remains high (about 30%).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_1-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_2-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_3-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_4-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_5-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_5-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_5-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/colourspace_68_5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="the-role-of-language">The role of language</h3> <p>To assess the role of language on categorical colour perception, we compare the results of four networks pretrained obtained from combination of two architectures:</p> <ul> <li><strong>Vision Transformer (ViT) – ViT-B32</strong></li> <li><strong>Convolution Neural Network (CNN) – ResNet50</strong></li> </ul> <p>trained on two different tasks:</p> <ul> <li><strong>CLIP (Multimodal language-vision)</strong></li> <li> <strong>ImageNet (Unimodal vision</strong>).</li> </ul> <p>For each of these networks, we have trained several instances to ensure the reported results are robust. Given the high degree of similarity across instances, below we visualise the results for instance 1.</p> <h4 id="clip---vit-b32">CLIP - ViT-B32</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_66_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_66_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_66_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_66_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="clip---resnet50">CLIP - ResNet50</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_68_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_68_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_68_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_68_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="imagenet---vit-b32">ImageNet - ViT-B32</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_70_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_70_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_70_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_70_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="imagenet---resnet50">ImageNet - ResNet50</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_72_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_72_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_72_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_72_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="multimodal-language-vision-vs-unimodal-vision">Multimodal language-vision vs. Unimodal vision</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/clip_vs_imagenet-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/clip_vs_imagenet-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/clip_vs_imagenet-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/clip_vs_imagenet.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="the-role-of-visual-task">The role of visual task</h3> <p>In this section, we examine how the network’s task shapes its colour categories using the <a href="http://taskonomy.stanford.edu/" rel="external nofollow noopener" target="_blank">Taskonomy dataset</a>, which contains about four million images (mainly indoor scenes from 2265 different buildings) and their corresponding labels for 25 computer vision tasks. The dataset also provides pretrained networks with an encoder-decoder architecture for all visual tasks. The encoder is the same across all pretrained networks (i.e., ResNet50), which maps the input image (\(224 \times 224\)) into a latent representation of size 1568 (\(14 \times 14 \times 8\)). The decoder design varies according to the dimensionality of the task output. The encoder offers a unique opportunity to study the role of visual tasks on the representation a network learns, given its architecture is identical across tasks and has been trained on the same set of images. Similar to ImageNet pretrained networks, we trained a linear classifier on top of the encoder’s extracted features from each Taskonomy pretrained network.</p> <h4 id="munsell-prediction">Munsell prediction</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_1-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_2-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_3-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_4-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_5-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_5-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_5-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_6-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_6-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_6-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_7-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_7-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_7-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_8-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_8-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_8-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_9-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_9-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_9-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_9.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_10-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_10-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_10-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_10.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_11-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_11-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_11-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_11.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_12-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_12-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_12-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_12.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_13-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_13-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_13-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_13.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_14-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_14-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_14-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_14.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_15-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_15-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_15-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_15.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_16-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_16-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_16-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_16.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_17-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_17-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_17-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_17.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_18-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_18-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_18-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_18.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_19-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_19-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_19-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_19.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_20-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_20-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_20-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_20.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_21-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_21-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_21-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_21.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_22-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_22-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_22-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_22.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_23-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_23-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_23-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_23.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_24-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_24-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_24-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_78_24.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="comparison-across-tasks">Comparison across tasks</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/taskonomy-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/taskonomy-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/taskonomy-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/taskonomy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="individual-colour-categories">Individual colour categories</h4> <p>We can compute the accuracy for each of the colour categories separately to analyse whether certain categories systematically obtain higher accuracy in comparison to the average. The figure below shows the individual colour categories averaged across all twenty-four tasks.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_90_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_90_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_90_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_90_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="rainbow-qualitative-prediction">Rainbow qualitative prediction</h3> <p>So far we have looked at the network’s prediction of Munsell chips and analysed them quantitatively. We can now qualitatively look at the network’s prediction of the rainbow image. Overall, we can observe that the networks’ prediction, at deeper layers, captures well our categorical perception. The language prediction particularly stands out. The only difference between the network’s prediction and my subjective colour perception is the colour of cyan. But of course, the network was never prompted with this colour term.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_93_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_93_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_93_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_93_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_94_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_94_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_94_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_94_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="additional-colour-terms">Additional colour terms</h4> <p>We tested the language models with an additional colour term <strong>cyan</strong> to check whether it could perfectly predict our perception of the rainbow. The results are presented in the figure below. We can see that both CLIP models, irrespective of the architecture of their vision encoder predict the cyan rings in between blue and green categories.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_97_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_97_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_97_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_97_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="discussion">Discussion</h2> <ul> <li> <strong>Unimodal vision</strong> models explain 85% of human data.</li> <li>The remaining 15% is explained by <strong>multimodal vision-language</strong> models.</li> <li>Categorical colour perception is a <strong>free-from-language representation</strong>, although its <strong>development</strong> is influenced by <strong>linguistic terms</strong>.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_83_0-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_83_0-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_83_0-1400.webp"></source> <img src="/assets/img/DeepReconciliationOfCategoricalColourPerception/output_83_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Arash Akbarinia. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-F9Z86194JJ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-F9Z86194JJ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>